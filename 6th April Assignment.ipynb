{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b42c0b2",
   "metadata": {},
   "source": [
    "### Q1. What is the mathematical formula for a linear SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736136b3",
   "metadata": {},
   "source": [
    "A linear support vector machine (SVM) is a binary classification algorithm that tries to find the best hyperplane that separates the two classes in a high-dimensional space. The mathematical formula for a linear SVM can be expressed as:\n",
    "\n",
    "                       f(x) = sign(w . x + b)\n",
    "                       \n",
    "where:\n",
    "\n",
    "- x is the input vector of dimension n\n",
    "- w is the weight vector of dimension n\n",
    "- b is the bias term\n",
    "- . denotes the dot product of w and x\n",
    "- sign is the sign function that returns 1 if the argument is positive, -1 if it's negative, and 0 if it's zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95eeca7f",
   "metadata": {},
   "source": [
    "### Q2. What is the objective function of a linear SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d62d7e5",
   "metadata": {},
   "source": [
    "The objective function of a linear SVM is to maximize the margin between the decision boundary (i.e., the hyperplane that separates the two classes) and the closest points from each class. The margin is defined as the distance between the decision boundary and the closest points from each class.\n",
    "\n",
    "minimize: 1/2 * ||w||^2\n",
    "subject to: y_i (w . x_i + b) >= 1 for i = 1, 2, ..., n\n",
    "\n",
    "where:\n",
    "\n",
    "- w is the weight vector\n",
    "- b is the bias term\n",
    "- x_i is the i-th input vector\n",
    "- y_i is the i-th label (+1 or -1)\n",
    "- n is the number of training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece912a0",
   "metadata": {},
   "source": [
    "### Q3. What is the kernel trick in SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bd2e2f",
   "metadata": {},
   "source": [
    "The kernel trick in SVM (Support Vector Machines) is a technique used to transform non-linearly separable data into a higher-dimensional space, where it becomes linearly separable. This is achieved by applying a non-linear transformation to the input data without explicitly computing the transformation. Instead, the transformation is done using a kernel function, which is a mathematical function that calculates the dot product of two points in the transformed space without computing the transformation explicitly.\n",
    "\n",
    "In SVM, the kernel trick is used to transform the input data into a higher-dimensional space, where it becomes linearly separable. The transformed data is then classified using a linear SVM, which can easily find the optimal hyperplane in the higher-dimensional space.\n",
    "\n",
    "The most commonly used kernel functions in SVM are:\n",
    "\n",
    "- Linear kernel: This kernel function is used when the data is already linearly separable.\n",
    "\n",
    "- Polynomial kernel: This kernel function is used when the data has a curved decision boundary.\n",
    "\n",
    "- Gaussian kernel (RBF): This kernel function is used when the data has a complex decision boundary that is not easy to define mathematically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba2ced2",
   "metadata": {},
   "source": [
    "### Q4. What is the role of support vectors in SVM Explain with example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56322b1a",
   "metadata": {},
   "source": [
    "Support vectors play a crucial role in SVMs (Support Vector Machines) as they define the decision boundary or the hyperplane that separates the two classes. These are the data points that are closest to the decision boundary and have the maximum influence on the position and orientation of the hyperplane.\n",
    "\n",
    "During the training process of an SVM, the algorithm tries to find the optimal hyperplane that maximizes the margin between the two classes while minimizing the classification error. However, not all the data points are equally important in determining the hyperplane. Only the support vectors, which are the data points closest to the hyperplane, are used to define the position and orientation of the hyperplane.\n",
    "\n",
    "Example\n",
    "Suppose we have a dataset of points in a two-dimensional space with two classes, A and B. The objective of the SVM algorithm is to find the optimal hyperplane that separates the two classes while maximizing the margin between them.\n",
    "\n",
    "During the training process, the algorithm identifies the support vectors, which are the data points that lie closest to the hyperplane. These support vectors determine the position and orientation of the hyperplane, while all other points are not relevant for the final decision boundary.\n",
    "\n",
    "For example, consider a dataset with three data points: two blue points representing class A and one red point representing class B. If we assume that the optimal hyperplane is a straight line, then the middle blue point and the red point are the support vectors. These two points define the hyperplane, while the other blue point is not a support vector and does not have any influence on the position and orientation of the hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61141c66",
   "metadata": {},
   "source": [
    "### Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b067b92",
   "metadata": {},
   "source": [
    "Hyperplane:\n",
    "The hyperplane is the decision boundary that separates the data points of two different classes. In a binary classification problem, the hyperplane is a line in 2D space or a plane in 3D space that separates the two classes. The hyperplane is determined by the SVM algorithm during the training process.\n",
    "\n",
    "Example: In the graph below, the hyperplane is a straight line that separates the two classes (blue and red) in a 2D space.\n",
    "\n",
    "![]\n",
    "\n",
    "\n",
    "Marginal plane:\n",
    "The marginal plane is a plane parallel to the hyperplane that is equidistant from the support vectors. The distance between the marginal plane and the hyperplane is called the margin. The margin is maximized in SVM to achieve better generalization performance.\n",
    "\n",
    "Example: In the graph below, the marginal plane is the dotted line parallel to the hyperplane and equidistant from the support vectors. The margin is the distance between the hyperplane and the marginal plane.\n",
    "\n",
    "\n",
    "\n",
    "Soft margin:\n",
    "The soft margin SVM allows some data points to be misclassified or fall within the margin. This is done to improve the generalization performance of the SVM algorithm.\n",
    "\n",
    "Example: In the graph below, the soft margin SVM allows some data points to be misclassified or fall within the margin. The misclassified points are shown as white circles, and the margin is represented by the dotted line. The soft margin SVM allows for some errors in the classification to achieve better generalization performance.\n",
    "\n",
    "\n",
    "\n",
    "Hard margin:\n",
    "The hard margin SVM does not allow any data points to be misclassified or fall within the margin. This is done to achieve better accuracy on the training set.\n",
    "\n",
    "Example: In the graph below, the hard margin SVM does not allow any data points to be misclassified or fall within the margin. The margin is represented by the dotted line, and all the data points are correctly classified. However, the hard margin SVM may not perform well on new data due to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1911bbf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
